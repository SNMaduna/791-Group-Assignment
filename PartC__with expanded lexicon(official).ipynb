{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58be1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install shap --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27da2b8-c579-4c04-a773-f35a3d0024ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import shap\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"expanded_lexicon_v4_working.xlsx\"  # Replace with your file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "languages = ['Sepedi', 'Xhosa', 'Shona', 'Afrikaans', 'Zulu']\n",
    "\n",
    "all_texts = []\n",
    "all_labels = []\n",
    "\n",
    "for lang in languages:\n",
    "    texts = df[lang].astype(str).tolist()\n",
    "    labels = df['Sentiment'].tolist()  # Use your sentiment column\n",
    "    all_texts.extend(texts)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "all_labels_encoded = le.fit_transform(all_labels)\n",
    "# Check the first few examples\n",
    "print(all_texts[:5])\n",
    "print(all_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437f9bd-9ffe-44c1-807c-fa4d4b730d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    all_texts, all_labels_encoded, test_size=0.2, random_state=42, stratify=all_labels_encoded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb678f1-9544-44bc-b5aa-e544697fdd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0206571-93e1-4f04-88d3-9cc86c764def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5️⃣ Load Models and Tokenizers\n",
    "# ==============================\n",
    "# AfroXLMR\n",
    "xlmr_name = \"Davlan/afro-xlmr-base\"\n",
    "tokenizer_xlmr = AutoTokenizer.from_pretrained(xlmr_name, use_fast=False)\n",
    "model_xlmr = AutoModelForSequenceClassification.from_pretrained(xlmr_name, num_labels=len(le.classes_))\n",
    "\n",
    "# AfriBERTa\n",
    "afri_name = \"castorini/afriberta_base\"\n",
    "tokenizer_afri = AutoTokenizer.from_pretrained(afri_name, use_fast=False)\n",
    "model_afri = AutoModelForSequenceClassification.from_pretrained(afri_name, num_labels=len(le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac479ff-61c5-459b-9bd8-907bf6d3b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tokenizer, train_texts, train_labels, epochs=1, batch_size=2, lr=2e-5):\n",
    "    dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} | Avg Loss: {total_loss / len(loader):.4f}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc72e865-aea0-4fa1-b864-697f4c827e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Models\n",
    "# ==============================\n",
    "print(\"Training AfroXLMR...\")\n",
    "model_xlmr = train_model(model_xlmr, tokenizer_xlmr, train_texts, train_labels, epochs=1)\n",
    "\n",
    "print(\"Training AfriBERTa...\")\n",
    "model_afri = train_model(model_afri, tokenizer_afri, train_texts, train_labels, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1b6bad-92ec-42fc-acbb-a32b146b0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explainable AI\n",
    "# ==============================\n",
    "# Create pipelines\n",
    "pipe_xlmr = pipeline(\"text-classification\", model=model_xlmr, tokenizer=tokenizer_xlmr, top_k=None)\n",
    "pipe_afri = pipeline(\"text-classification\", model=model_afri, tokenizer=tokenizer_afri, top_k=None)\n",
    "\n",
    "# Small sample for SHAP\n",
    "sample_texts = test_texts[:3]\n",
    "\n",
    "explainer_xlmr = shap.Explainer(pipe_xlmr)\n",
    "explainer_afri = shap.Explainer(pipe_afri)\n",
    "\n",
    "print(\"Explaining AfroXLMR predictions...\")\n",
    "shap_values_xlmr = explainer_xlmr(sample_texts)\n",
    "shap.plots.text(shap_values_xlmr[0])\n",
    "\n",
    "print(\"Explaining AfriBERTa predictions...\")\n",
    "shap_values_afri = explainer_afri(sample_texts)\n",
    "shap.plots.text(shap_values_afri[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8eef7-f8de-43d7-b5d1-e148cb43b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Learning\n",
    "# ==============================\n",
    "def get_probs(pipe, texts):\n",
    "    preds = pipe(texts, top_k=None)\n",
    "    return np.array([[score['score'] for score in p] for p in preds])\n",
    "\n",
    "print(\"Getting predictions from AfroXLMR...\")\n",
    "probs_xlmr = get_probs(pipe_xlmr, test_texts)\n",
    "\n",
    "print(\"Getting predictions from AfriBERTa...\")\n",
    "probs_afri = get_probs(pipe_afri, test_texts)\n",
    "\n",
    "# Soft-voting ensemble\n",
    "ensemble_probs = (probs_xlmr + probs_afri) / 2\n",
    "ensemble_preds = ensemble_probs.argmax(axis=1)\n",
    "\n",
    "# Evaluate\n",
    "acc = accuracy_score(test_labels, ensemble_preds)\n",
    "print(f\"\\nEnsemble Accuracy (multilingual): {acc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, ensemble_preds, target_names=le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
