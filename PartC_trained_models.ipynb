{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "addb1370-630c-4b18-b280-e24e4e0bc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1️⃣ Load and Prepare Data\n",
    "# ==========================================\n",
    "df = pd.read_csv(\"cleaned_corpus_gpt_SUBTOKENIZED.csv\")\n",
    "\n",
    "# Use multilingual text and label columns\n",
    "texts = df[\"text_hybrid\"].astype(str).tolist()\n",
    "labels = df[\"label\"].astype(str).tolist()\n",
    "\n",
    "# Encode labels to integers\n",
    "unique_labels = sorted(set(labels))\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "numeric_labels = [label2id[l] for l in labels]\n",
    "\n",
    "# Train-test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, numeric_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c116d4fd-4e0a-4447-b5bf-28bbd4fedaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ Dataset and DataLoader\n",
    "# ==========================================\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65bd4797-1ae8-40a3-b40e-c785aac6e5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at castorini/afriberta_large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 3️⃣ Model Setup\n",
    "# ==========================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device set to use {device}\")\n",
    "\n",
    "# AfroXLMR\n",
    "xlmr_name = \"Davlan/afro-xlmr-base\"\n",
    "tokenizer_xlmr = AutoTokenizer.from_pretrained(xlmr_name)\n",
    "model_xlmr = AutoModelForSequenceClassification.from_pretrained(\n",
    "    xlmr_name, num_labels=len(unique_labels), id2label=id2label, label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "# AfriBERTa\n",
    "afri_name = \"castorini/afriberta_large\"\n",
    "tokenizer_afri = AutoTokenizer.from_pretrained(afri_name)\n",
    "model_afri = AutoModelForSequenceClassification.from_pretrained(\n",
    "    afri_name, num_labels=len(unique_labels), id2label=id2label, label2id=label2id\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60ab5bf-4cda-4dc3-b488-7a8fd341c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ Training Function\n",
    "# ==========================================\n",
    "def train_model(model, tokenizer, train_texts, train_labels, epochs=1, batch_size=8, lr=1e-5):\n",
    "    dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46d73f36-4320-4d99-bb6d-df7cc2faa193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AfroXLMR...\n",
      "Epoch 1 | Loss: 0.9996\n",
      "Training AfriBERTa...\n",
      "Epoch 1 | Loss: 1.0046\n"
     ]
    }
   ],
   "source": [
    "# 5️⃣ Train Both Models\n",
    "# ==========================================\n",
    "print(\"Training AfroXLMR...\")\n",
    "model_xlmr = train_model(model_xlmr, tokenizer_xlmr, train_texts, train_labels, epochs=1)\n",
    "\n",
    "print(\"Training AfriBERTa...\")\n",
    "model_afri = train_model(model_afri, tokenizer_afri, train_texts, train_labels, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f854949c-5648-4675-9878-92cde9f598e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explaining AfroXLMR...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_xlmr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExplaining AfroXLMR...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m sample_texts = test_texts[:\u001b[32m5\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m explainer_xlmr = shap.Explainer(\u001b[38;5;28;01mlambda\u001b[39;00m x: model_xlmr(**tokenizer_xlmr(x, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m)).logits.detach().numpy(), \u001b[43mtokenizer_xlmr\u001b[49m)\n\u001b[32m      6\u001b[39m shap_values_xlmr = explainer_xlmr(sample_texts)\n\u001b[32m      7\u001b[39m shap.plots.text(shap_values_xlmr[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer_xlmr' is not defined"
     ]
    }
   ],
   "source": [
    "# 6️⃣ Explainable AI (XAI) - SHAP Explanations\n",
    "# ==========================================\n",
    "print(\"\\nExplaining AfroXLMR...\")\n",
    "sample_texts = test_texts[:5]\n",
    "explainer_xlmr = shap.Explainer(lambda x: model_xlmr(**tokenizer_xlmr(x, return_tensors=\"pt\", truncation=True, padding=True)).logits.detach().numpy(), tokenizer_xlmr)\n",
    "shap_values_xlmr = explainer_xlmr(sample_texts)\n",
    "shap.plots.text(shap_values_xlmr[0])\n",
    "\n",
    "print(\"\\nExplaining AfriBERTa...\")\n",
    "explainer_afri = shap.Explainer(lambda x: model_afri(**tokenizer_afri(x, return_tensors=\"pt\", truncation=True, padding=True)).logits.detach().numpy(), tokenizer_afri)\n",
    "shap_values_afri = explainer_afri(sample_texts)\n",
    "shap.plots.text(shap_values_afri[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84f811a2-c7bd-4c06-897d-10cfa0f50811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('trained_afriberta_tokenizer\\\\tokenizer_config.json',\n",
       " 'trained_afriberta_tokenizer\\\\special_tokens_map.json',\n",
       " 'trained_afriberta_tokenizer\\\\sentencepiece.bpe.model',\n",
       " 'trained_afriberta_tokenizer\\\\added_tokens.json',\n",
       " 'trained_afriberta_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the trained models\n",
    "\n",
    "model_xlmr.save_pretrained(\"trained_afroxlmr_model\")\n",
    "tokenizer_xlmr.save_pretrained(\"trained_afroxlmr_tokenizer\")\n",
    "\n",
    "model_afri.save_pretrained(\"trained_afriberta_model\")\n",
    "tokenizer_afri.save_pretrained(\"trained_afriberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0657f05-99a1-49ba-99fc-ea5c091175f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load trained models \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer_xlmr = AutoTokenizer.from_pretrained(\"trained_afroxlmr_tokenizer\")\n",
    "model_xlmr = AutoModelForSequenceClassification.from_pretrained(\"trained_afroxlmr_model\")\n",
    "\n",
    "tokenizer_afri = AutoTokenizer.from_pretrained(\"trained_afriberta_tokenizer\")\n",
    "model_afri = AutoModelForSequenceClassification.from_pretrained(\"trained_afriberta_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
