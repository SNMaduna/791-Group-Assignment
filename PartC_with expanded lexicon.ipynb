{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b384e45d-7f03-4d79-8920-4e044cb6617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedc4e4-1696-48b2-b397-c576424637b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59282ecf7ada45c9b337d2d504ba8d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/446M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AfroXLMR\n",
    "xlmr_model_name = \"Davlan/afro-xlmr-base\"\n",
    "tokenizer_xlmr = AutoTokenizer.from_pretrained(xlmr_model_name, use_fast=False)\n",
    "model_xlmr = AutoModelForSequenceClassification.from_pretrained(xlmr_model_name, num_labels=3)\n",
    "model_xlmr.eval()\n",
    "\n",
    "# AfriBERTa\n",
    "afri_model_name = \"castorini/afriberta_base\"\n",
    "tokenizer_afri = AutoTokenizer.from_pretrained(afri_model_name, use_fast=False)\n",
    "model_afri = AutoModelForSequenceClassification.from_pretrained(afri_model_name, num_labels=3)\n",
    "model_afri.eval()\n",
    "\n",
    "\n",
    "lexicon = pd.read_excel(r\"C:\\Users\\HP\\OneDrive\\Desktop\\expanded_lexicon_v4.xlsx\")\n",
    "\n",
    "# Choose a target language column (e.g., Zulu)\n",
    "texts = lexicon['Zulu'].astype(str)\n",
    "labels = lexicon['Sentiment'].map({'negatif': 0, 'neuitre': 1, 'positif': 2})\n",
    "\n",
    "# Split into train and test sets\n",
    "train_enc_xlmr = tokenizer_xlmr(train_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_enc_xlmr  = tokenizer_xlmr(test_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "train_enc_afri = tokenizer_afri(train_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_enc_afri  = tokenizer_afri(test_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset_xlmr = TensorDataset(train_enc_xlmr['input_ids'], train_enc_xlmr['attention_mask'], torch.tensor(train_labels))\n",
    "eval_dataset_xlmr  = TensorDataset(test_enc_xlmr['input_ids'], test_enc_xlmr['attention_mask'], torch.tensor(test_labels))\n",
    "\n",
    "train_dataset_afri = TensorDataset(train_enc_afri['input_ids'], train_enc_afri['attention_mask'], torch.tensor(train_labels))\n",
    "eval_dataset_afri  = TensorDataset(test_enc_afri['input_ids'], test_enc_afri['attention_mask'], torch.tensor(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc242d86-a9ce-4368-8f99-2303276d0aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4️⃣ Define metrics\n",
    "# ========================\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# ========================\n",
    "# 5️⃣ TrainingArguments (low-memory friendly)\n",
    "# ========================\n",
    "training_args_xlmr = TrainingArguments(\n",
    "    output_dir=\"./results_xlmr\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    logging_dir=\"./logs_xlmr\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "training_args_afri = TrainingArguments(\n",
    "    output_dir=\"./results_afri\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    logging_dir=\"./logs_afri\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# 6️⃣ Create Trainers\n",
    "# ========================\n",
    "trainer_xlmr = Trainer(\n",
    "    model=model_xlmr,\n",
    "    args=training_args_xlmr,\n",
    "    train_dataset=train_dataset_xlmr,\n",
    "    eval_dataset=eval_dataset_xlmr,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_afri = Trainer(\n",
    "    model=model_afri,\n",
    "    args=training_args_afri,\n",
    "    train_dataset=train_dataset_afri,\n",
    "    eval_dataset=eval_dataset_afri,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# 7️⃣ Train models\n",
    "# ========================\n",
    "trainer_xlmr.train()\n",
    "trainer_afri.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bca63-5160-411e-8bba-1bcd421c99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XAI\n",
    "\n",
    "# AfroXLMR\n",
    "def predict_xlmr(texts):\n",
    "    enc = tokenizer_xlmr(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model_xlmr(**enc)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1).numpy()\n",
    "    return probs\n",
    "\n",
    "# AfriBERTa\n",
    "def predict_afri(texts):\n",
    "    enc = tokenizer_afri(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model_afri(**enc)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1).numpy()\n",
    "    return probs\n",
    "    background_texts = train_texts[:5] \n",
    "\n",
    "    # Create explainers using partition algorithm (lightweight)\n",
    "explainer_xlmr = shap.Explainer(predict_xlmr, background_texts, algorithm=\"partition\")\n",
    "explainer_afri = shap.Explainer(predict_afri, background_texts, algorithm=\"partition\")\n",
    "\n",
    "sample_texts = test_texts  # or select a few examples if memory is tight\n",
    "\n",
    "shap_values_xlmr = explainer_xlmr(sample_texts)\n",
    "shap_values_afri  = explainer_afri(sample_texts)\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"Text: {text}\\n--- AfroXLMR ---\")\n",
    "    shap.plots.text(shap_values_xlmr[i])\n",
    "    print(f\"--- AfriBERTa ---\")\n",
    "    shap.plots.text(shap_values_afri[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a1a1d-ed90-4731-a2ac-a2fbf84677d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble\n",
    "\n",
    "# Get probabilities from each model\n",
    "probs_xlmr = predict_xlmr(test_texts)\n",
    "probs_afri = predict_afri(test_texts)\n",
    "\n",
    "# Average probabilities (soft-voting)\n",
    "ensemble_probs = (probs_xlmr + probs_afri) / 2\n",
    "ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
    "\n",
    "# Evaluate ensemble\n",
    "acc = accuracy_score(test_labels, ensemble_preds)\n",
    "f1 = f1_score(test_labels, ensemble_preds, average=\"weighted\")\n",
    "\n",
    "print(\"Ensemble Predictions:\", ensemble_preds)\n",
    "print(f\"Ensemble Accuracy: {acc:.3f}, F1-score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42864fd-4c44-49f2-a9e8-a361c725ac91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
